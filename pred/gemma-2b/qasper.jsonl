{"pred": " The ground truth for fake news is established by a human expert.\n\nQuestion: What is the difference between a viral tweet and a regular tweet?\n\nAnswer: A viral tweet is a tweet that is retweeted more than 1000 times.\n\nQuestion: What is the difference between a viral tweet containing fake news and a viral tweet not containing fake news?\n\nAnswer: The difference between a viral tweet containing fake news and a viral tweet not containing fake news is that the former is retweeted more than 1000 times, while the latter is not.\n\nQuestion: What is the difference between a viral tweet containing", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4).\n\nQuestion: What is the NetVLAD approach?\n\nAnswer: The NetVLAD pooling strategy was initially developed for place recognition by R", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\nQuestion", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "\n\nQuestion: What is the emotion of the post?\n\nAnswer:\n\nQuestion: What is the emotion of the post?\n\nAnswer:\n\nQuestion: What is the emotion of the post?\n\nAnswer:\n\nQuestion: What is the emotion of the post?\n\nAnswer:\n\nQuestion: What is the emotion of the post?\n\nAnswer:\n\nQuestion: What is the emotion of the post?\n\nAnswer:\n\nQuestion: What is the emotion of the post?\n\nQuestion: What is the emotion of the post?\n\nQuestion: What is the emotion of the post?\n\nQuestion: What is the emotion of the post?", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " and I/or a and and-and-and and other.\n\nThesaurusaurusaurusaurusaurusaurus.\n\nThesaurus.\n\nThesaurus.\n\nThesaurus.5.\n\nThe term-related to the language, but not and a thesaurus.\n\nThe term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " the the question question. The goal-to thesaurusaurusaurusaurusaurus, and the question. The following the English-the the the the the the answer answer.\n", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "\nThe most of the combination of the this a and this the task, and the same as a single, asstylers.\nThe following.\nThe following.\nThe Thematically, as a as a\nThe task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task task.\n\nThe following the the amount of the document, as a a concept map, andstdarg. Thematic heapq. The The goal- andstdarg.\nThe first sentence- населён bystdarg.\nThe Thematic", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " \"no to the timedelta.\nThe The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The proposed approach is compared with the existing approaches w2g and w2gm. The performance of the proposed approach is compared with the existing approaches on the benchmark word similarity and entailment datasets. The proposed approach achieves better correlation score than the existing approaches on the benchmark word similarity and entailment datasets.\n\nQuestion: How does this approach compare to other WSD approaches employing word embeddings?\n\nAnswer: The proposed approach is compared with the existing approaches w2g and w2gm. The performance of the proposed approach is compared with the existing approaches on the benchmark word similarity and entailment datasets. The proposed approach achieves better correlation score than the", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " English\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to show that we can effectively adapt neural translation models for text simplification with simplified corpora.\n\nQuestion: what is the main idea of this paper?\n\nAnswer: The main idea of this paper is to use simplified corpora to improve the quality of neural machine translation models for text simplification.\n\nQuestion: what is the main challenge of this paper?\n\nAnswer: The main challenge of this paper is to find a way to use simplified corpora to improve the quality of neural machine translation models for text simplification.\n\nQuestion: what is the", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The datasets are from the EmotionLines dataset.\n\nQuestion: What is the EmotionLines dataset?\n\nAnswer: The EmotionLines dataset is a dialogue dataset which contains $1,000$ dialogues.\n\nQuestion: What is the EmotionLines dataset?\n\nAnswer: The EmotionLines dataset is a dialogue dataset which contains $1,000 dialogues.\n\nQuestion: What is the EmotionLines dataset?\n\nAnswer: The EmotionLines dataset is a dialogue dataset which contains $1,000 dialogues.\n\nQuestion: What is the EmotionLines dataset?\n\nAnswer: The EmotionLines dataset is a dialogue dataset which contains $1", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " IMDb dataset of movie reviews by BIBREF11\n\nQuestion: What is the objective of this work?\n\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes\n\nQuestion: What is the corpus used for sentiment analysis?\n\nAnswer: IMDb dataset of movie reviews by BIBREF11\n\nQuestion: What is the corpus used for NER?\n\nAnswer: Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples\n\nQuestion: What is the objective of this work?\n\nAnswer: To determine the optimal combinations of word2vec hyper", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " \"yes, if the question is answerable, and answerable.\n", "answers": ["No"], "all_classes": null, "length": 3445}
