{"pred": " The ground truth for fake news is established by manually labeling a dataset of tweets as containing fake news or not containing fake news.\n\nAnswer: The ground truth for fake news is established by analyzing the content of the tweets and determining if they contain false information.\n\nAnswer: The ground truth for fake news is established by comparing the number of retweets for tweets containing fake news and tweets not containing fake news.\n\nAnswer: The ground truth for fake news is established by analyzing the number of followers and friends of accounts spreading fake news and comparing it to accounts spreading viral content.\n\nAnswer: The ground truth for fake news", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " The GhostVLAD approach is a pooling strategy used for language identification task. It was originally proposed for face recognition and is inspired by the work done by W. Xie et al. [9] and Y. Zhong et al. [10]. It uses Ghost-VLAD to improve the accuracy of language identification task for Indian languages.\n\nQuestion: How does the GhostVLAD approach improve the accuracy of language identification?\n\nAnswer: The GhostVLAD approach improves the accuracy of language identification by aggregating the frame-level features into a single utterance level feature. It uses a pooling strategy that is inspired by", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " Their model outperforms the state of the art results by 68.8% to 71.8%.\n\nQuestion: What is the main weakness of the TRE model?\n\nAnswer: The main weakness of the TRE model is that it incorrectly predicts instances of the sad class as the happy class.\n\nQuestion: What is the main strength of the MDRE model?\n\nAnswer: The main strength of the MDRE model is that it compensates for the weaknesses of the previous two models and benefits from their strengths to a surprising degree.\n\nQuestion: What is the main goal of the proposed model?\n\nAnswer:", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": " The article mentions that the use of context tweets generally have little effect on baseline models, however they noticeably improve the scores of several metrics. For instance, CNN with context tweets score the highest recall and F1 for “hateful\" labels, and RNN models with context tweets have the highest recall for “abusive\" tweets.\n\nAnswer: The article mentions that the most accurate in classifying abusive language is the LR model followed by ensemble models such as GBT and RF. Character-level representations improve F1 scores of SVM and RF classifiers, but they have no positive effect on other models.\n\nAnswer", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " They looked at different Facebook pages.\n\nQuestion: What did they use to train their models?\n\nAnswer: They used Facebook reactions as labels to train their models.\n\nQuestion: What is the name of their best model?\n\nAnswer: Their best model is called B-M.\n\nQuestion: What is the name of the feature set they used?\n\nAnswer: They used all the features plus Google-based embeddings.\n\nQuestion: What is the name of the dataset they used for evaluation?\n\nAnswer: They used three datasets for evaluation: the Affective HMM, the Affect", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " No, the hashtag and SemEval datasets contain data in multiple languages.\n\nQuestion: What is the purpose of hashtag segmentation?\n\nAnswer: The purpose of hashtag segmentation is to automatically split a hashtag into meaningful word sequences, which can improve the performance of downstream tasks such as sentiment analysis.\n\nQuestion: How does the new dataset for hashtag segmentation differ from the previous dataset?\n\nAnswer: The new dataset for hashtag segmentation includes segmentation for 12,594 unique hashtags and their associated tweets, which is a significant improvement over the previous dataset of 1,108 hashtags.\n\nQuestion: What", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " The proposed task is concept-map-based MDS, which is defined as creating a concept map that represents the most important content of a set of related documents, satisfies a specified size limit, and is connected.\n\nQuestion: What is the purpose of the corpus creation method described in the article?\n\nAnswer: The corpus creation method is used to create a new benchmark corpus for concept-map-based MDS. It combines automatic preprocessing, scalable crowdsourcing, and high-quality expert annotations to overcome the challenges of creating a dataset for this task.\n\nQuestion: How does the corpus differ from traditional summarization corpor", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " The datasets used for evaluation are CNN/DailyMail, NYT, and XSum.\n\nQuestion: What is the purpose of the QA evaluation?\n\nAnswer: The purpose of the QA evaluation is to quantify the degree to which summarization models retain key information from the document.\n\nQuestion: What is the scoring mechanism used for quality-based evaluation?\n\nAnswer: The scoring mechanism used for quality-based evaluation is the percentage of times a system is chosen as better minus the times it is selected as worse.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " This approach, GM$\\_$KL, achieves better correlation than existing approaches for various metrics on the SCWS dataset. It also performs better than w2g and w2gm approaches on the benchmark word similarity datasets.\n\nQuestion: What is the objective function used in this approach?\n\nAnswer: The objective function used in this approach is a variant of max-margin objective based on the asymmetric KL divergence energy function.\n\nQuestion: What is the purpose of using a KL divergence approximation in this approach?\n\nAnswer: The purpose of using a KL divergence approximation in this approach is to handle the intract", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " Their ensemble method works by averaging the predictions from the constituent single models. They start with the best performing model according to validation performance and then try adding the best performing model that has not been previously tried. They keep it in the ensemble if it improves its validation performance and discard it otherwise. This way they gradually try each model once. They call the resulting model a greedy ensemble. They used the INLINEFORM0 BookTest validation dataset for this procedure.\n\nQuestion: What is the best learning rate for their model?\n\nAnswer: The best learning rate for their model was INLINEFORM3.\n\nQuestion: What is the", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The datasets are Friends and EmotionPush.\n\nQuestion: What are the emotions that can be predicted by the proposed models?\n\nAnswer: The proposed models can predict the emotions of Joy, Sadness, Anger, and Neutral.\n\nQuestion: What is the difference between single utterance and dialogue emotion recognition?\n\nAnswer: Single utterance emotion recognition only considers the emotion of a single utterance, while dialogue emotion recognition considers the emotion of a series of utterances contributed by a conversation.\n\nQuestion: What is the objective of the challenge?\n\nAnswer: The objective of the challenge is to predict the emotion", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " The paper focuses on the English language.\n\nQuestion: What is the main goal of automatic text simplification?\n\nAnswer: The main goal of automatic text simplification is to reduce the lexical and structural complexity of a text while still retaining the semantic meaning, which can help children, non-native speakers, and people with cognitive disabilities to understand text better.\n\nQuestion: What are the three categories of automatic text simplification methods?\n\nAnswer: The three categories of automatic text simplification methods are lexical simplification, rule-based simplification, and machine translation.\n\nQuestion: What is the main difference", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The IMDb dataset of movie reviews by BIBREF11.\n\nQuestion: What is the objective of this work?\n\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for intrinsic evaluation (semantic and syntactic analogies) and extrinsic evaluation tasks, like SA and NER.\n\nQuestion: What is the difference between continuous skipgram and CBoW?\n\nAnswer: Continuous skipgram predicts (by maximizing classification of) words before and after the center word, for a given range. Since distant words are less connected to a center word in a sentence, less weight is", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " The proposed system achieves a significant improvement in accuracy compared to the baseline systems.\n\nQuestion: What is the impact of character embeddings in the proposed system?\n\nAnswer: The impact of character embeddings in the proposed system is significant, as the model with pretrained embeddings outperforms the model with random embeddings.\n\nQuestion: What are the two examples used in the case studies?\n\nAnswer: The two examples used in the case studies are from the DL-PS dataset.\n\nQuestion: What is the reason why the model of LSTM-CRF fails to recognize the named", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " Yes, they conducted an extensive validation to ensure the quality of the recorded data.\n\nQuestion: What is the purpose of the Zurich Cognitive Language Processing Corpus (ZuCo) 2.0?\n\nAnswer: The purpose of the ZuCo 2.0 is to provide simultaneous eye movement and brain activity recordings during natural reading and annotation.\n\nQuestion: What are the applications and re-use potentials of this data?\n\nAnswer: The provided word-level and sentence-level eye-tracking and EEG features can be used to improve and evaluate NLP and machine learning methods, for instance, to evaluate linguistic phenomena", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " The datasets used are the dataset of the Brazilian National Bank (BNDES) and the dataset of the Brazilian Central Bank (BCB).\n\nQuestion: What is the purpose of the hybrid conceptual architecture?\n\nAnswer: The purpose of the hybrid conceptual architecture is to provide a framework for designing and engineering MPCS.\n\nQuestion: What is the main challenge in MPCS?\n\nAnswer: The main challenge in MPCS is the coordination and mediation among chatbots and humans.\n\nQuestion: What is the purpose of the micro-service implementation of SABIA?\n\nAnswer: The purpose of the micro-", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The article does not provide information on which stock market sector achieved the best performance.\n\nAnswer: The article does not provide information on which stock market sector achieved the best performance.\n\nAnswer: The article does not provide information on which stock market sector achieved the best performance.\n\nAnswer: The article does not provide information on which stock market sector achieved the best performance.\n\nAnswer: The article does not provide information on which stock market sector achieved the best performance.\n\nAnswer: The article does not provide information on which stock market sector achieved the best performance.\n\nAnswer: The article does not provide information on which", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " They compared the performance of SMT models and various NMT models.\n\nQuestion: What is the average length of the translation results of the three models?\n\nAnswer: The average length of the translation results of the three models are: RNN-based NMT:17.12, SMT:15.50, Transformer:16.78, Reference:16.47.\n\nQuestion: What is the accurate rate of translating `Zhen'?\n\nAnswer: The accurate rate of translating `Zhen' are: RNN-based NMT:0.14, SMT:0.16,", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": " The three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to investigate into the problem of how to leverage prior knowledge robustly in learning models.\n\nQuestion: What is the difference between the maximum entropy regularization term and the KL divergence regularization term?\n\nAnswer: The maximum entropy regularization term assumes that the categories are uniformly distributed,", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " The baselines are SVM with n-gram or average word embedding feature, CNN, and RCNN.\n\nQuestion: What is the purpose of the UTCNN model?\n\nAnswer: The purpose of the UTCNN model is to classify stances on social media posts by incorporating user, topic, content, and comment information.\n\nQuestion: What is the advantage of using user embeddings in the UTCNN model?\n\nAnswer: The advantage of using user embeddings in the UTCNN model is that they capture the semantics of the users and their interactions, which can improve the accuracy of stance classification.\n", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " They improved by several points.\n\nQuestion: What is the name of the winning system of the 2016 edition of the challenge?\n\nAnswer: The winning system of the 2016 edition of the challenge is called \"Balikas et al.\"\n\nQuestion: What is the name of the state-of-the-art system of the 2016 edition of the challenge?\n\nAnswer: The state-of-the-art system of the 2016 edition of the challenge is called \"Balikas et al.\"\n\nQuestion: What is the name of the neural network architecture that uses multitask learning?\n\nAnswer: The", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " Their model improves interpretability by using sparse attention and adaptive span transformers, which allow for a more precise and dynamic selection of relevant words.\n\nQuestion: What is the purpose of the multi-head attention mechanisms in the Transformer?\n\nAnswer: The multi-head attention mechanisms in the Transformer allow for a more efficient and context-dependent strategy for propagating information within and across sentences.\n\nQuestion: What is the role of the attention distribution in the Transformer?\n\nAnswer: The attention distribution in the Transformer determines which words are relevant and which are not, and it can be controlled by using sparse normal", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The baseline was a context-agnostic MT system.\n\nQuestion: what was the key idea of the approach?\n\nAnswer: The key idea was to use monolingual data to imitate typical inconsistencies between context-agnostic translations of isolated sentences.\n\nQuestion: what was the performance of the model?\n\nAnswer: The model showed substantial improvements in translation quality as measured by BLEU, targeted contrastive evaluation of several discourse phenomena and human evaluation.\n\nQuestion: what was the limitation of previous work?\n\nAnswer: Previous work assumed that all training data was at the document level.\n\nQuestion:", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": " The metrics used for evaluation are natural language inference, document classification, named entity recognition, part-of-speech tagging, and dependency parsing.\n\nQuestion: What is the advantage of using multilingual models?\n\nAnswer: Multilingual models enable many NLP applications for other languages via zero-shot transfer.\n\nQuestion: What is the main contribution of the work?\n\nAnswer: The main contribution of the work is the development of a technique to rapidly transfer an existing pre-trained model from English to other languages in an energy efficient way.\n\nQuestion: What is the performance of the bilingual LM on six languages on", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module is pretrained on a large-scale MT dataset.\n\nQuestion: What is the role of the speech encoder in pre-training and fine-tuning?\n\nAnswer: In pre-training, the speech encoder plays the role of a pure acoustic model, while in fine-tuning, it has to extract semantic and linguistic features.\n\nQuestion: What is the main difference between our model and previous works?\n\nAnswer: Our model reuses all subnets in pre-training, keeps the roles of subnets consistent, and pre-trains the attention module.\n\nQuestion:", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " The stylistic features obtained are Unigrams and Pragmatic features BIBREF0, BIBREF1, BIBREF2, BIBREF3, Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5.\n\nAnswer: The cognitive features, along with textual features used in best available sarcasm detectors, are used to train binary classifiers against given sarcasm labels. Our experiments show significant improvement in classification accuracy over the state of the art, by performing such augmentation.\n\nAnswer: The cognitive features, along with textual features used in best available sarcasm detectors, are", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The encoder has an LSTM.\n\nQuestion: What is the purpose of the MSD prediction task?\n\nAnswer: The purpose of the MSD prediction task is to predict the morphosyntactic description of the lemma.\n\nQuestion: What is the main difference between the baseline system and the system described in the article?\n\nAnswer: The main difference between the baseline system and the system described in the article is that the system described in the article has a wider context window and uses multi-task learning.\n\nQuestion: What is the purpose of the multilingual training and monolingual finet", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " Yes, WordNet is useful for taxonomic reasoning for this task.\n\nQuestion: What is the main appeal of using automatically generate datasets?\n\nAnswer: The main appeal of using automatically generate datasets is the ability to systematically manipulate and control the complexity of target questions, which allows for more controlled experimentation and new forms of evaluation.\n\nQuestion: What is the main drawback of using synthetic versus naturalistic QA data?\n\nAnswer: The main drawback of using synthetic versus naturalistic QA data is that it is much harder to validate the quality of such data at such a scale and such varying levels of complexity.\n\n", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " The baselines were the best performing models on the LibriSpeech and 2000hr Fisher+Switchboard tasks.\n\nQuestion: what was the performance of Jasper on the WSJ dataset?\n\nAnswer: Jasper achieved SOTA performance on the test-clean subset and SOTA among end-to-end speech recognition models on test-other on the WSJ dataset.\n\nQuestion: what was the performance of Jasper on the Hub5'00 dataset?\n\nAnswer: Jasper achieved good results for the Switchboard subset, but there is work to be done to improve WER on harder tasks such as the Callhome subset", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": " They look at over 20,000 users.\n\nQuestion: What is the overall accuracy of their classifier?\n\nAnswer: The overall accuracy is up to 0.534.\n\nQuestion: What is the gender dominance ratio in the Banking industry?\n\nAnswer: The gender dominance ratio in the Banking industry is not statistically significantly correlated with the usage of positive (or negative) emotional words in either gender in their dataset.\n\nQuestion: What is the name of the dictionary they use to measure the emotional orientation of a text?\n\nAnswer: They use the Linguistic Inquiry and Word Count (LIWC)", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " The metrics used for evaluation are recipe-level coherence, local coherence, and human preference.\n\nQuestion: What is the purpose of the personalized models?\n\nAnswer: The purpose of the personalized models is to generate personalized and coherent recipes based on user preferences.\n\nQuestion: How many personalized models are there?\n\nAnswer: There are three personalized models: Prior Tech, Prior Name, and Prior Recipe.\n\nQuestion: What is the difference between the personalized models?\n\nAnswer: The personalized models differ in how they incorporate user preferences. Prior Tech models use user preferences extracted from previously consumed recipes, Prior Name", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " They create labels for the extracted clinical information from the symptom monitoring conversations.\n\nQuestion: What is the motivation behind designing a dialogue comprehension system?\n\nAnswer: The motivation is to extract key clinical information from spoken conversations between nurses and patients in telehealth settings.\n\nQuestion: What are the challenges in designing a dialogue comprehension system for human-human spoken conversations?\n\nAnswer: The challenges include zero anaphora, thinking aloud, and topic drift.\n\nQuestion: How do they evaluate the performance of their dialogue comprehension system?\n\nAnswer: They evaluate the performance using exact match (EM) and F1 score metrics", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": " The task-specific encoder is trained on the entire corpus, which consists of 5k abstracts.\n\nQuestion: What is the purpose of the universal sentence encoder?\n\nAnswer: The universal sentence encoder is used to learn a fixed-length representation of each sentence in the corpus.\n\nQuestion: What is the main finding of the experiments on biomedical information extraction tasks?\n\nAnswer: The main finding is that adding expert annotations for difficult instances significantly improves model performance, and that a combination of expert and crowd annotations is better than relying solely on crowd annotations.\n\nQuestion: What is the significance of the difficulty", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": " The tasks used for evaluation are BERT BIBREF3, GPT-2 BIBREF4, and BERT BIBREF5.\n\nQuestion: What is the purpose of the Transformer architecture?\n\nAnswer: The purpose of the Transformer architecture is to map an input sequence to an output sequence through hierarchical multi-head attention mechanisms, yielding a dynamic, context-dependent strategy for propagating information within and across sentences.\n\nQuestion: What is the role of attention heads in the Transformer architecture?\n\nAnswer: Attention heads in the Transformer architecture capture various relationships between tokens, such as flatter", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the main problem with word2vec embeddings?\n\nAnswer: The main problem with word2vec embeddings is their failure to express polysemous words.\n\nQuestion: What is the purpose of the ELMo model?\n\nAnswer: The purpose of the ELMo model is to introduce a contextual component and capture the context of words.\n\nQuestion: What is the evaluation metric used to measure the performance of ELMo embeddings on the NER task?\n\nAnswer: The evaluation metric used", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " The authors of the article have diverse disciplinary backgrounds and have worked in various fields related to computational text analysis. They have experience in the humanities and social sciences, which have focused on the challenges of textual analysis.\n\nQuestion: What are the challenges faced in computational analysis of textual data?\n\nAnswer: The challenges in computational analysis of textual data include the cultural and social context of texts, the subtleties of meaning and interpretation, and the conceptual validity of operationalizations. These challenges require careful consideration and refinement throughout the research process.\n\nQuestion: How can computational text analysis help in understanding social and cultural phenomena?\n\nAnswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " Yes, this paper is introducing an unsupervised approach to spam detection.\n\nQuestion: What are the two topic-based features extracted in this paper?\n\nAnswer: The two topic-based features extracted in this paper are GOSS and LOSS.\n\nQuestion: What is the purpose of the LOSS+GOSS features?\n\nAnswer: The purpose of the LOSS+GOSS features is to effectively detect \"smart\" spammers who post seemingly legitimate tweets and are thus difficult to identify by existing spammer classification methods.\n\nQuestion: What is the main advantage of the LOSS+GOSS features", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": " The Nguni languages are similar to each other.\n\nQuestion: What is the focus of this section?\n\nAnswer: The focus of this section is on recently published datasets and LID research applicable to the South African context.\n\nQuestion: What is the proposed algorithm for LID?\n\nAnswer: The proposed algorithm is a hierarchical naive Bayesian and lexicon based classifier for LID of short pieces of text of 15-20 characters long.\n\nQuestion: What is the accuracy of the proposed algorithm?\n\nAnswer: The accuracy of the proposed algorithm seems to be dependent on the support of the lex", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " They compared with deep lstm models.\n\nQuestion: what is the purpose of the linear classifier probe?\n\nAnswer: The purpose of the linear classifier probe is to better understand the dynamics inside a neural network.\n\nQuestion: what is the difference between deep lstm and shallow lstm?\n\nAnswer: Deep lstm is more effective in improving the performance, while shallow lstm is easier to train.\n\nQuestion: what is the difference between smbr and ce?\n\nAnswer: smbr is a method that can achieve linear speedup, while ce is a", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The data set is not specified in the article.\n\nQuestion: What is the purpose of their research?\n\nAnswer: The purpose of their research is to assess the quality of documents using a combination of textual and visual features.\n\nQuestion: What is the accuracy of their joint model?\n\nAnswer: The accuracy of their joint model is state-of-the-art over 3/4 of their datasets.\n\nQuestion: What is the main contribution of their research?\n\nAnswer: The main contribution of their research is the ability to achieve better accuracy on document quality assessment by complementing textual features with visual features", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " The human judgements were assembled by the authors of the article.\n\nQuestion: What is the main drawback of neural translation models involving morphologically rich languages?\n\nAnswer: The main drawback of neural translation models involving morphologically rich languages is the heavy corpus requirement in order to ensure learning of deeper contexts.\n\nQuestion: What is the main difference between languages like English and morphologically rich languages?\n\nAnswer: The main difference between languages like English and morphologically rich languages is that morphologically rich languages are structurally and semantically discordant from languages like English.\n\nQuestion: What is the purpose of morphological segment", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " No, the article does not mention any specific language pairs that they test their framework performance on.\n\nQuestion: What is the purpose of the proposed approach in the paper?\n\nAnswer: The purpose of the proposed approach is to seamlessly extend the original NMT framework to multilingual settings.\n\nQuestion: What are the two demanding scenarios that they evaluate their systems in?\n\nAnswer: The two demanding scenarios are under-resourced translation and zero-resourced translation.\n\nQuestion: What is the difference between the direct system and the pivot system?\n\nAnswer: The direct system is an NMT trained on German", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " Models are evaluated based on their efficiency and accuracy in the communication game.\n\nQuestion: What are the two rule-based baselines used in the experiments?\n\nAnswer: The two rule-based baselines used in the experiments are Unif and Stopword.\n\nQuestion: What is the purpose of the user study mentioned in the article?\n\nAnswer: The purpose of the user study is to measure completion times and accuracies for typing randomly sampled sentences from the Yelp corpus.\n\nQuestion: What is the main finding of the user study?\n\nAnswer: The main finding of the user study is that the aut", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " The evaluation metrics looked at for classification tasks are ROUGE BIBREF22 unigram score.\n\nQuestion: What is the purpose of the Performance Appraisal (PA) process?\n\nAnswer: The purpose of the Performance Appraisal (PA) process is to measure and evaluate every employee's performance.\n\nQuestion: What is the role of the Performance Appraisal (PA) system in the PA process?\n\nAnswer: The Performance Appraisal (PA) system is used to record the interactions that happen in various steps of the PA process.\n\nQuestion: What is the role of automated", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The source domain is Book (B) and the target domain is Beauty (K).\n\nQuestion: How many unlabeled reviews are allowed to be used for both the source and the target domains?\n\nAnswer: 4000 unlabeled reviews are allowed to be used for both the source and the target domains.\n\nQuestion: What is the classifier trained on?\n\nAnswer: The classifier is trained on the training set of the source domain.\n\nQuestion: What is the objective of the proposed method?\n\nAnswer: The objective of the proposed method is to jointly perform feature adaptation and semi-supervised", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": " They compare with LSTMs and similar recurrent units.\n\nQuestion: what is the impact of using the pyramidal transformation for the input vectors?\n\nAnswer: It improves the perplexity by about 1 point on both the PTB and WT-2 datasets while reducing the number of recurrent unit parameters by about 14%.\n\nQuestion: what is the impact of using the grouped linear transformation for context vectors?\n\nAnswer: It reduces the total number of recurrent unit parameters by about 75% while the performance drops by about 11%.\n\nQuestion: what is the impact of using the convolution-based sub-sampling", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": " NeuronBlocks includes several common neural network modules such as embedding, CNN/RNN, Transformer, and more.\n\nQuestion: How does NeuronBlocks improve the productivity of engineers?\n\nAnswer: NeuronBlocks improves the productivity of engineers by providing a two-layer solution that targets three types of engineers. The Model Zoo consists of various templates for the most common NLP tasks, while the Block Zoo supplies a gallery of alternative layers/modules for the networks. This design achieves a balance between generality and flexibility, making it easier for engineers to develop NLP DNN approaches.\n\nQuestion: What is the", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " They used Phoible BIBREF4 and URIEL BIBREF5.\n\nQuestion: what is the purpose of the language ID token?\n\nAnswer: The language ID token is used to identify the language of the source sentence in multilingual g2p systems.\n\nQuestion: what is the difference between the language embeddings and the phoneme embeddings?\n\nAnswer: The language embeddings are learned from a large multilingual corpus and are used to identify the language of the source sentence. The phoneme embeddings are learned from the IPA and are used to identify the phonemes", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " The baselines were simple rule-based systems (BIBREF3) based on linguistic information extracted from the sentences.\n\nQuestion: What were the results for the negation cue detection and scope resolution task?\n\nAnswer: The results for the negation cue detection and scope resolution task were state-of-the-art on BF, BA and SFU datasets.\n\nQuestion: What was the gain on the speculation detection task when trained on multiple datasets?\n\nAnswer: The gain on the speculation detection task when trained on multiple datasets was 10.6 F1 points on BF and 1.94 F1 points on", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " The languages they use in their experiment are English, Spanish, French, German, Italian, and Portuguese.\n\nAnswer: The languages they use in their experiment are English, Spanish, French, German, Italian, and Portuguese.\n\nAnswer: The languages they use in their experiment are English, Spanish, French, German, Italian, and Portuguese.\n\nAnswer: The languages they use in their experiment are English, Spanish, French, German, Italian, and Portuguese.\n\nAnswer: The languages they use in their experiment are English, Spanish, French, German, Italian, and Portuguese.\n\nAnswer: The languages they", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": " They test their method on tracking infectious diseases.\n\nQuestion: What is the name of the neural network model they use?\n\nAnswer: The neural network model they use is called tweet2vec.\n\nQuestion: What is the name of the website they use to get their data?\n\nAnswer: The website they use to get their data is called Twitter.\n\nQuestion: What is the name of the person who provided them with access to the Twitter data?\n\nAnswer: The person who provided them with access to the Twitter data is Juergen Pfeffer.\n\nQuestion: What is the name of", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Yes, they use pretrained embeddings.\n\nQuestion: What is the purpose of the bifocal attention mechanism?\n\nAnswer: The purpose of the bifocal attention mechanism is to compute an attention over fields at a macro level and over values at a micro level.\n\nQuestion: What is the stay on behavior?\n\nAnswer: The stay on behavior is when the model pays attention to a field (say, occupation) and stays on it for a few timesteps (till all the occupations are produced in the output).\n\nQuestion: What is the never look back behavior?\n\nAnswer", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " No, the article does not mention any evaluation against a baseline.\n\nQuestion: What is the purpose of the PolyResponse system?\n\nAnswer: The purpose of the PolyResponse system is to assist users in finding relevant information and performing tasks, such as restaurant search and booking.\n\nQuestion: How does the PolyResponse system handle multiple relevant responses?\n\nAnswer: The PolyResponse system shows one response from each relevant restaurant, and when there are multiple relevant restaurants, it shows one response from each.\n\nQuestion: What is the role of the intent classifiers in the PolyResponse system?\n\nAnswer: The intent", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": " They obtain psychological dimensions of people by analyzing the language used in the blog posts.\n\nQuestion: What is the purpose of the blog collection?\n\nAnswer: The purpose of the blog collection is to create insightful mappings of the blogging community.\n\nQuestion: What is the main constraint for accepting blogs in the collection?\n\nAnswer: The main constraint for accepting blogs in the collection is that they must have specific location information.\n\nQuestion: What is the purpose of the prototype demo?\n\nAnswer: The purpose of the prototype demo is to allow users to draw maps of the geographical distributions on different LIWC categories.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": " The ML methods aim to identify argument components such as propositions, reasons, evidence, and fallacies.\n\nAnswer: The ML methods aim to identify argument components such as propositions, reasons, evidence, and fallacies.\n\nAnswer: The ML methods aim to identify argument components such as propositions, reasons, evidence, and fallacies.\n\nAnswer: The ML methods aim to identify argument components such as propositions, reasons, evidence, and fallacies.\n\nAnswer: The ML methods aim to identify argument components such as propositions, reasons, evidence, and fallacies.\n\nAnswer: The ML methods aim to identify argument components", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " PARENT aligns ngrams of length INLINEFORM7.\n\nQuestion: What is the precision of PARENT?\n\nAnswer: PARENT has a precision of INLINEFORM3.\n\nQuestion: What is the recall of PARENT?\n\nAnswer: PARENT has a recall of INLINEFORM3.\n\nQuestion: What is the F-score of PARENT?\n\nAnswer: PARENT has an F-score of INLINEFORM3.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the development of the metric", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": " The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5 million comments.\n\nAnswer: The Twitter dataset is approximately 1.5", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": " The 12 languages covered are Mandarin, Russian, French, German, Italian, Turkish, Farsi, Finnish, Welsh, Kiswahili, Yue Chinese, and English.\n\nQuestion: What is the purpose of the Multi-SimLex dataset?\n\nAnswer: The purpose of the Multi-SimLex dataset is to provide a suite of manually and consistently annotated semantic datasets for 12 different languages, focused on the fundamental lexical relation of semantic similarity.\n\nQuestion: How does the Multi-SimLex dataset compare to previous semantic similarity datasets?\n\nAnswer: The Multi-SimLex dataset is larger and more comprehensive", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": " The model is applied to two datasets: Wikipedia and CMV.\n\nQuestion: What is the main challenge in adapting these models to the supervised domain of conversational forecasting?\n\nAnswer: The main challenge is the relative scarcity of labeled data: at most a few thousands.\n\nQuestion: What is the main insight in this work?\n\nAnswer: The main insight is that models with these properties already exist, albeit geared toward generation rather than prediction: recent work in context-aware dialog generation (or “chatbots”) has proposed sequential neural models that make effective use of the intra-conversational dynamics B", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " No, none of the pipeline components were based on deep learning models.\n\nQuestion: What is the main goal of the proposed architecture?\n\nAnswer: The main goal of the proposed architecture is to be modular and not rely on human made rules, allowing for its independence from a specific language.\n\nQuestion: What is the purpose of the lexicon matching module?\n\nAnswer: The purpose of the lexicon matching module is to link words found in the text source with the data available on Eurovoc BIBREF6 thesaurus and IATE BIBREF7 terminology database.\n\nQuestion: What is the main", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": " The quality of the data is empirically evaluated by calculating the BLEU scores for the machine translation models trained on the CoVoST corpus. The BLEU scores are used to measure the performance of the models in terms of their ability to translate the speech from one language to another. The BLEU scores are also used to compare the performance of different models and different language pairs.\n", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": " They combine audio and text sequences using a feed-forward neural model.\n\nQuestion: What is the purpose of the attention mechanism in their model?\n\nAnswer: The attention mechanism is used to uncover enhanced learning schemes that will increase performance in both speech emotion recognition and other multimodal classification tasks.\n\nQuestion: What is the accuracy of their model when applied to the IEMOCAP dataset?\n\nAnswer: The accuracy of their model when applied to the IEMOCAP dataset ranges from 68.8% to 71.8%.\n\nQuestion: What is the main issue that their model resolves in previous models?", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": " Their model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI compared with other models, except Dress on FKGL and SBMT-SARI on SARI.\n\nQuestion: What is the main reason for the effectiveness of their method?\n\nAnswer: The main reason for the effectiveness of their method is that they do not change the neural network architecture to integrate simplified corpora.\n\nQuestion: What is the impact of not only the quality of the NTS system used for back-translation, but also the amount of available parallel and simplified corpora?\n\nAnswer", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": " The article does not mention the number of humans who evaluated the results.\n\nQuestion: What is the main limitation of previous work on automatic post-editing?\n\nAnswer: The main limitation of previous work on automatic post-editing is that it assumes access to parallel document-level training data.\n\nQuestion: What is the purpose of the DocRepair model?\n\nAnswer: The purpose of the DocRepair model is to correct inconsistencies between sentence-level translations of a context-agnostic MT system.\n\nQuestion: What is the key contribution of the current work?\n\nAnswer: The key contribution", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": " Their definition of tweets going viral is tweets that have a high number of retweets.\n\nQuestion: What is the main goal of their research?\n\nAnswer: The main goal of their research is to identify features of tweets containing fake news in order to develop mechanisms to block them.\n\nQuestion: What is the main finding related to the time of exposure?\n\nAnswer: The main finding related to the time of exposure is that viral tweets containing fake news are shorter-lived than those containing other type of content.\n\nQuestion: What is the main finding related to the accounts spreading fake news?\n\nAnswer:", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": " BERT performs best by itself.\n\nQuestion: What is the difference between fake news and propaganda?\n\nAnswer: Propaganda is built upon true information, while fake news is not.\n\nQuestion: What is the purpose of the postprocess step in the SLC task?\n\nAnswer: The purpose of the postprocess step is to check for repetition propaganda technique by computing cosine similarity between the current sentence and its preceding $w=10$ sentence vectors.\n\nQuestion: What is the purpose of the multi-task neural sequence tagger in the FLC task?\n\nAnswer: The purpose of the multi", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": " The data was collected using crowdsourcing.\n\nQuestion: what is the main goal of the DeepMine project?\n\nAnswer: The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the performance of the DeepMine database in terms of speech recognition?\n\nAnswer: The DeepMine database can be used to train well performing and practically usable Persian ASR models.\n\nQuestion: what is the difference between the DeepMine database and the RSR2015 and RedDots databases?\n\nAnswer: The Deep", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": " The article mentions that several machine learning and deep learning methods are used for RQE, including Logistic Regression, Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs).\n\nQuestion: What is the goal of RQE?\n\nAnswer: The goal of RQE is to retrieve answers to a premise question (INLINEFORM0 ) by retrieving inferred or entailed questions, called hypothesis questions (INLINEFORM1 ) that already have associated answers.\n\nQuestion: What is the entailment relation between two questions?\n", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " The benchmark dataset is the one collected by Lee et al. (2011) and it has been extensively explored in our paper. Its quality is high.\n\nQuestion: What is the main difference between content polluters and fake accounts?\n\nAnswer: Content polluters mainly focus on certain topics, while fake accounts focus on a wide range of topics due to random copying and retweeting of other users' tweets.\n\nQuestion: What is the main advantage of our proposed features over other features?\n\nAnswer: Our proposed features have a great advantage over other features in terms of recall and F1-score.\n\nQuestion:", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": " The decoder has an LSTM.\n\nQuestion: What is the name of the shared task?\n\nAnswer: The shared task is called CoNLL–SIGMORPHON 2018.\n\nQuestion: What is the name of the task that the system is designed for?\n\nAnswer: The system is designed for Task 2 of the CoNLL–SIGMORPHON 2018 shared task.\n\nQuestion: What is the name of the system that the authors are comparing their system to?\n\nAnswer: The authors are comparing their system to the baseline system.\n\nQuestion: What", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " No, they report results on both English and Chinese data.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the proposed Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction.\n\nQuestion: What is the principle idea of the proposed approach?\n\nAnswer: The principle idea of the proposed approach is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).\n\nQuestion: What", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": " The best performing model among author's submissions is the ensemble+ of (r4, r7 r12) with a performance of 0.673 on dev (external).\n\nAnswer: The best performing model among author's submissions is the ensemble+ of (r4, r7 r12) with a performance of 0.673 on dev (external).\n\nAnswer: The best performing model among author's submissions is the ensemble+ of (r4, r7 r12) with a performance of 0.673 on dev (external).\n\nAnswer: The best performing model among author's submissions is the ensemble+ of (", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " The baseline was a strong NMT model trained from scratch on the mixture of six-way pseudo-parallel data generated by VII and the original parallel data.\n\nAnswer: The baseline was a strong NMT model trained from scratch on the mixture of six-way pseudo-parallel data generated by VII and the original parallel data.\n\nAnswer: The baseline was a strong NMT model trained from scratch on the mixture of six-way pseudo-parallel data generated by VII and the original parallel data.\n\nAnswer: The baseline was a strong NMT model trained from scratch on the mixture of six-way pseudo-", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " Their highest recall score was achieved in the fourth test batch set.\n\nQuestion: What is the name of the neural network based model proposed by Wiese et al.?\n\nAnswer: The neural network based model proposed by Wiese et al. is called Fast QA.\n\nQuestion: What is the name of the competition where BioASQ organizers provide the training and testing data?\n\nAnswer: The competition where BioASQ organizers provide the training and testing data is called BioASQ.\n\nQuestion: What is the name of the model proposed by Dimitriadis et al. for factoid question", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " The paper explores word embedding techniques such as word2vec.\n\nQuestion: What is the goal of the proposed method?\n\nAnswer: The goal of the proposed method is to quantify the similarity and relatedness between two terms by integrating pair–wise similarity scores into second–order vectors.\n\nQuestion: How does the proposed method reduce noise in the vectors?\n\nAnswer: The proposed method reduces noise in the vectors by selecting term pairs from biomedical text based on their semantic similarity and giving larger weights to more similar pairs.\n\nQuestion: What datasets are used to evaluate the proposed method?\n\nAnswer: The proposed", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " They use a bilingual dictionary to translate each word in the source language into English.\n\nQuestion: What is the purpose of pre-ordering the assisting language?\n\nAnswer: The purpose of pre-ordering the assisting language is to match the word order of the source language and reduce word order divergence.\n\nQuestion: What is the impact of word order divergence on translation?\n\nAnswer: Word order divergence can limit the benefits of multilingual translation and lead to the generation of unknown words in the test output.\n\nQuestion: How do they reduce the number of unknown tokens in the test output?\n\nAnswer: They reduce", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": " No, the paper does not explore extraction from electronic health records.\n\nQuestion: What is the main problem in BioIE?\n\nAnswer: The main problem in BioIE is the diversity of the various techniques from Information Extraction that have been applied in the Biomedical domain.\n\nQuestion: What is the main difference between the approaches in BIBREF4 and BIBREF5?\n\nAnswer: The main difference between the approaches in BIBREF4 and BIBREF5 is that BIBREF4 does not require any prepared dictionary, while BIBREF5 uses a machine learning approach.\n\nQuestion:", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": " The experts used for annotation were domain experts with legal training.\n\nQuestion: What is the goal of the PrivacyQA corpus?\n\nAnswer: The goal of the PrivacyQA corpus is to kickstart the development of question-answering methods for the privacy domain, where advances in enabling users to quickly identify the privacy issues most salient to them can potentially have large real-world impact.\n\nQuestion: What are some challenges to answering questions about privacy policies?\n\nAnswer: Some challenges to answering questions about privacy policies include the fact that many users do not read privacy policies, questions can be ill-formed or vague", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " The models used for painting embedding are CNN-RNN based image-to-poem net and the models used for language style transfer are seq2seq model with parallel text corpus and seq2seq model with pointer networks.\n\nQuestion: What is the average content score across the paintings?\n\nAnswer: The average content score across the paintings is 3.7.\n\nQuestion: What is the average style score across the paintings?\n\nAnswer: The average style score across the paintings is 3.9.\n\nQuestion: What is the average creativity score across the paintings?\n\nAnswer: The average creativity score", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": " The RNN layer works better than the transformer layer.\n\nQuestion: What is the main issue with Transformers in dealing with long sequences?\n\nAnswer: Transformers are able to consume only a limited context of symbols as their input.\n\nQuestion: What are the two methods proposed in the paper to improve the performance of BERT on long documents?\n\nAnswer: The two methods proposed in the paper are RoBERT and ToBERT.\n\nQuestion: What is the main difference between RoBERT and ToBERT?\n\nAnswer: RoBERT splits the input text sequence into shorter segments and uses another layer", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": " Yes.\n\nQuestion: What is the name of the neural-network-based MRC model proposed in the article?\n\nAnswer: BiDAF BIBREF3.\n\nQuestion: What is the name of the data enrichment method proposed in the article?\n\nAnswer: WordNet-based data enrichment method.\n\nQuestion: What is the name of the end-to-end MRC model proposed in the article?\n\nAnswer: Knowledge Aided Reader (KAR).\n\nQuestion: What is the name of the knowledge base used in the article?\n\nAnswer: WordNet.\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " They addressed three topics of cyberbullying: personal attack, racism, and sexism.\n\nQuestion: What is the difference between TL1 and TL2?\n\nAnswer: TL1 directly uses a model trained on one dataset to detect cyberbullying in other datasets, while TL2 only transfers learned word embeddings from one dataset to another.\n\nQuestion: What is the main bottleneck in previous works on cyberbullying detection?\n\nAnswer: The main bottlenecks are targeting only one particular social media platform, addressing only one topic of cyberbullying, and relying on handcrafted features.\n\nQuestion:", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " They obtain the new context representation by splitting the contexts into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, they want to focus on it but not ignore the other regions completely. Hence, they propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, they force the network to pay special attention", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " The dataset contains different types of entities like PERSON, LOCATION, ORGANIZATION, and others.\n\nAnswer: The dataset contains different types of entities like PERSON, LOCATION, ORGANIZATION, and others.\n\nAnswer: The dataset contains different types of entities like PERSON, LOCATION, ORGANIZATION, and others.\n\nAnswer: The dataset contains different types of entities like PERSON, LOCATION, ORGANIZATION, and others.\n\nAnswer: The dataset contains different types of entities like PERSON, LOCATION, ORGANIZATION, and others.\n\nAnswer: The dataset contains", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": " The resulting annotated data is significantly higher quality.\n\nQuestion: What is the main finding of the study?\n\nAnswer: The main finding of the study is that mixing expert and crowd annotations leads to better models than using solely crowd data, and better than using expert data alone.\n\nQuestion: What is the purpose of the study?\n\nAnswer: The purpose of the study is to investigate the effectiveness of mixing expert and crowd annotations for biomedical information extraction.\n\nQuestion: What is the significance of the study?\n\nAnswer: The significance of the study is that it provides insights into the importance of annotation quality for", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": " The imbalance in analyzed corpora is significant, with 65% of the speakers being men and speaking more than 75% of the time.\n\nQuestion: What is the impact of the observed disparity on ASR performance?\n\nAnswer: The observed disparity in gender representation has a significant impact on ASR performance, with a WER increase of 24% for women compared to men.\n\nQuestion: Is this as simple as a problem of gender proportion in the training data or are other factors entangled?\n\nAnswer: This is not as simple as a problem of gender proportion in the training data. Other factors, such as speaker's", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " The approach achieves state of the art results on the Multi30K dataset BIBREF21.\n\nQuestion: What is the purpose of the Multi30K dataset?\n\nAnswer: The purpose of the Multi30K dataset is to provide English (EN) descriptions taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23, BIBREF24, BIBREF25.\n\nQuestion: What is the difference between the results for German and French?\n\nAnswer: The results for German show that the deliberation models are overall better than the base", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": " Strong baselines model is compared to BIBREF9, BIBREF11, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34, BIBREF35, BIBREF36, BIBREF37, BIBREF38, B", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": " Statistical machine learning models.\n\nAnswer: Cyber security, political elections, and public health.\n\nAnswer: Weak supervision, expectation regularization, and crowdsourcing.\n\nAnswer: Cyber security, political elections, and public health.\n\nAnswer: Weak supervision, expectation regularization, and crowdsourcing.\n\nAnswer: Weak supervision, expectation regularization, and crowdsourcing.\n\nAnswer: Weak supervision, expectation regularization, and crowdsourcing.\n\nAnswer: Weak supervision, expectation regularization, and crowdsourcing.\n\nAnswer: Weak supervision, expectation regularization, and crowdsourcing.\n\nAnswer: Weak", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " They use NLTK, Stanford CoreNLP, TwitterNLP, BIBREF14, BIBREF15, BIBREF16, BIBREF17, and BIBREF18.\n\nQuestion: What is the CCR?\n\nAnswer: The CCR stands for correct classification rate, which is the number of correctly predicted sentiments or entities over the number of groundtruth sentiments or entities in a set of tweets.\n\nQuestion: What is the average CCR for the crowdworkers?\n\nAnswer: The average CCR for the crowdworkers is 98.6%.\n\nQuestion: What is the", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": " Experiments are performed on SQuAD BIBREF3.\n\nQuestion: What is the purpose of question generation?\n\nAnswer: The purpose of question generation is to automatically create questions from a range of inputs, such as natural language text, knowledge base, and image.\n\nQuestion: What is the main difference between rule-based and neural models for question generation?\n\nAnswer: The main difference between rule-based and neural models for question generation is that rule-based models use predefined rules or templates, while neural models use sequence-to-sequence learning frameworks.\n\nQuestion: What is the main challenge", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and identifying itineraries.\n\nQuestion: what is the main hypothesis in this paper?\n\nAnswer: The main hypothesis in this paper is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main motivation for using vector space embeddings?\n\nAnswer: The main motivation for using vector space embeddings is that they allow us to integrate the textual information we get from Flickr with", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": " Yes, they use attention.\n\nQuestion: What is the purpose of the model?\n\nAnswer: The purpose of the model is to handle unanswerable questions and be optimized jointly.\n\nQuestion: What is the contribution of the work?\n\nAnswer: The contribution of the work is summarized as follows:\n\n- They propose a simple yet efficient model for MRC that handles unanswerable questions and is optimized jointly.\n- Their model achieves competitive results on SQuAD v2.0.\n\nQuestion: What is the model?\n\nAnswer: The model is a variation of SAN BIBREF", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": " They used three datasets for evaluation: CSAT, 20newsgroups, and Fisher.\n\nAnswer: They used three datasets for evaluation: CSAT, 20newsgroups, and Fisher.\n\nAnswer: They used three datasets for evaluation: CSAT, 20newsgroups, and Fisher.\n\nAnswer: They used three datasets for evaluation: CSAT, 20newsgroups, and Fisher.\n\nAnswer: They used three datasets for evaluation: CSAT, 20newsgroups, and Fisher.\n\nAnswer: They used three datasets for evaluation: CSAT, 20newsgroups, and Fisher.\n\nAnswer: They used", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": " The sentiment classification dataset used is the IMDB dataset.\n\nQuestion: What is the purpose of the convolutional component in a QRNN?\n\nAnswer: The purpose of the convolutional component in a QRNN is to allow fully parallel computation across both minibatch and spatial dimensions, in this case the sequence dimension.\n\nQuestion: What is the purpose of the pooling component in a QRNN?\n\nAnswer: The purpose of the pooling component in a QRNN is to allow fully parallel computation across minibatch and feature dimensions.\n\nQuestion: What is the purpose of the convolutional", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": " Yes, BIBREF1, BIBREF2, and BIBREF3 were evaluated in previous work.\n\nQuestion: What is the difference between the BERT models and the LSTM models in terms of their ability to capture syntax-sensitive structures?\n\nAnswer: The BERT models rely purely on attention mechanisms and do not have an explicit notion of word order, while the LSTM models model word order directly and track states across the sentence.\n\nQuestion: What is the main reason for the high performance of the BERT models?\n\nAnswer: The BERT models are trained on a larger and", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": " Yes, the dataset used for sentiment analysis is balanced.\n\nQuestion: What is the accuracy of existing NLP systems for sentiment analysis?\n\nAnswer: The accuracy of existing NLP systems for sentiment analysis is low.\n\nQuestion: Can existing NLP systems accurately perform sentiment analysis of political tweets?\n\nAnswer: No, existing NLP systems cannot accurately perform sentiment analysis of political tweets.\n\nQuestion: What is the CCR for crowdworkers in the named-entity recognition experiment?\n\nAnswer: The CCR for crowdworkers in the named-entity recognition experiment is 98.6%.\n\nQuestion: What", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": " The invertibility condition is a requirement for the proposed generative approach to leverage continuous word representations for unsupervised learning of syntactic structure. It ensures that the model can be inverted to obtain the original input sequence.\n\nQuestion: How does the proposed approach capture the syntactic properties of language?\n\nAnswer: The proposed approach captures the syntactic properties of language by learning a new latent embedding space as a projection of pre-trained embeddings. This latent embedding space is more suitable for the syntax model, allowing for a better representation of syntactic categories and dependencies.\n\nQuestion: What are some related work", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " The proposed qualitative annotation schema is based on linguistic features that were introduced in the GLUE suite and the reasoning categories introduced in the WorldTree dataset.\n\nQuestion: What are the potential applications of the proposed framework?\n\nAnswer: The proposed framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach.\n\nQuestion: What are the issues with the factual correctness of the gold standards?\n\nAnswer: The gold standards suffer from flaws in design and contain overly specific keywords.\n\nQuestion: What is the goal of the proposed framework?\n\n", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": " The sizes of both datasets are WikiLarge and WikiSmall.\n\nQuestion: what is the name of the neural machine translation system used in the experiments?\n\nAnswer: The name of the neural machine translation system used in the experiments is OpenNMT.\n\nQuestion: what is the name of the method that replaces difficult words with more common words?\n\nAnswer: The name of the method that replaces difficult words with more common words is lexical simplification.\n\nQuestion: what is the name of the method that uses a deep reinforcement learning framework?\n\nAnswer: The name of the method that uses a deep reinforcement", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": " The baselines are the existing approaches for speech-to-text translation (ST) that can be categorized into cascaded method and end-to-end method.\n\nQuestion: What is the main difference between the cascaded method and the end-to-end method?\n\nAnswer: The main difference between the cascaded method and the end-to-end method is that the cascaded method translates outputs of an automatic speech recognition (ASR) system into target language, while the end-to-end method translates the acoustic speech into text in target language without yielding the source transcription.\n\nQuestion: What are the", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": " The paper studies Natural Language Processing (NLP) and Machine Learning (ML).\n\nQuestion: What is the main problem addressed in this paper?\n\nAnswer: The main problem addressed in this paper is the challenges of imbalanced classification and the ability to generalize on dissimilar data.\n\nQuestion: What is the name of the language model used in this paper?\n\nAnswer: The language model used in this paper is called BERT.\n\nQuestion: What is the name of the shared task dataset used in this paper?\n\nAnswer: The shared task dataset used in this paper is called PTC corpus.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": " The experiment uses a CNN-based sentence classifier and a BiLSTM model.\n\nQuestion: What is the purpose of the experiment?\n\nAnswer: The purpose of the experiment is to train systems to identify offensive language in social media.\n\nQuestion: What is the performance of the CNN system in the experiment?\n\nAnswer: The CNN system achieved higher performance compared to the BiLSTM model, with a macro-F1 score of 0.69.\n\nQuestion: What is the distribution of the labels in the dataset?\n\nAnswer: The distribution of the labels in the dataset is described in", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " Yes.\n\nQuestion: Do the answered questions measure for the quality of the answer?\n\nAnswer: Yes.\n\nQuestion: Do the answered questions measure for the readability of the answer?\n\nAnswer: Yes.\n\nQuestion: Do the answered questions measure for the engagement of the answer?\n\nAnswer: Yes.\n\nQuestion: Do the answered questions measure for the relevance of the answer?\n\nAnswer: Yes.\n\nQuestion: Do the answered questions measure for the accuracy of the answer?\n\nAnswer: Yes.\n\nQuestion: Do the answered questions measure for the timeliness of", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " The paper uses various pretrained word embeddings such as GloVe, Edinburgh, and Word2Vec.\n\nQuestion: What is the purpose of the preprocessing step?\n\nAnswer: The preprocessing step modifies the raw tweets before they are passed to feature extraction. It includes replacing username handles to USERNAME, phone numbers to PHONENUMBER, numbers to NUMBER, URLs to URL and times to TIME. It also breaks continuous sequence of emojis into individual tokens and converts all tokens to lowercase.\n\nQuestion: What are the three major categories of feature extractors used in the", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": " They showed that their personalized models can generate plausible, personalized, and coherent recipes preferred by human evaluators for consumption.\n\nQuestion: What is the main contribution of their work?\n\nAnswer: They propose a new task of generating personalized recipes from incomplete input specifications by leveraging historical user preferences.\n\nQuestion: What is the difference between their approach and previous works?\n\nAnswer: They attend over historical user preferences from previously consumed recipes to generate recipe content, rather than writing styles.\n\nQuestion: What are the challenges of their approach?\n\nAnswer: They have to deal with the potentially unconstrained number of ingredients", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": " The combination of rewards for reinforcement learning is a pre-training process and reinforcement learning.\n\nAnswer: The combination of rewards for reinforcement learning is a pre-training process and reinforcement learning.\n\nAnswer: The combination of rewards for reinforcement learning is a pre-training process and reinforcement learning.\n\nAnswer: The combination of rewards for reinforcement learning is a pre-training process and reinforcement learning.\n\nAnswer: The combination of rewards for reinforcement learning is a pre-training process and reinforcement learning.\n\nAnswer: The combination of rewards for reinforcement learning is a pre-training process and reinforcement learning.\n\nAnswer: The combination", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": " The authors demonstrate that their model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": " They compared their models to the following systems:\n\n- B-M model on development data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n- B-M model on test data\n", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users.\n\nAnswer: They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users.\n\nAnswer: They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users.\n\nAnswer: They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users.\n\nAnswer: They found significant differences on the distribution of followers, the number of URLs on tweets", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": " The dataset of hashtags is sourced from the SemEval 2017 shared task.\n\nQuestion: What is the goal of the study?\n\nAnswer: The goal of the study is to build efficient methods for automatically splitting a hashtag into a meaningful word sequence.\n\nQuestion: What are the three categories of current approaches for hashtag segmentation?\n\nAnswer: The three categories of current approaches for hashtag segmentation are gazeteer and rule based, word boundary detection, and ranking with language model and other features.\n\nQuestion: What is the current state-of-the-art approach for hashtag segmentation?\n", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": " The article does not mention any specific accents present in the corpus.\n\nQuestion: How many speakers are included in the corpus?\n\nAnswer: The article does not mention the exact number of speakers included in the corpus.\n\nQuestion: What is the purpose of the DeepMine project?\n\nAnswer: The purpose of the DeepMine project is to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: What is the main goal of the DeepMine project?\n\nAnswer: The main goal of the DeepMine project is to collect speech from at least a few thousand", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Word subspace can represent the semantic meaning of words and their context.\n\nQuestion: What is the main contribution of the proposed word subspace formulation?\n\nAnswer: The main contribution of the proposed word subspace formulation is that it can effectively and compactly represent the context of the corresponding text.\n\nQuestion: What is the difference between the word subspace formulation and the bag-of-words approach?\n\nAnswer: The word subspace formulation considers the semantic meaning of words and their context, while the bag-of-words approach only considers the frequency of words.\n\nQuestion: What is the significance of using", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": " The baseline model used is S1, based on lexical similarity.\n\nQuestion: What is the overall performance of the baseline model?\n\nAnswer: The overall performance of the baseline model is P=0.43, R=0.43, and F1=0.43.\n\nQuestion: What is the impact of the proposed approach on expanding entity profiles?\n\nAnswer: The proposed approach is able to expand both long-tail and trunk entities, with the exception of the class Creative Work.\n\nQuestion: What is the main goal of the proposed approach?\n\nAnswer: The main goal of the", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": " No, SemCor3.0 is not reflective of English language data in general.\n\nQuestion: What is the main difference between knowledge-based and supervised methods in WSD?\n\nAnswer: The main difference between knowledge-based and supervised methods in WSD is that knowledge-based methods rely on lexical resources like WordNet and exploit two kinds of lexical knowledge, while supervised methods focus on extracting manually designed features and train a dedicated classifier for each target lemma.\n\nQuestion: What is the advantage of using BERT in our approach?\n\nAnswer: The advantage of using BERT in our approach is", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": " The Augmented LibriSpeech dataset is the largest speech-to-text translation corpus to date, with over 11,000 speakers and over 60 accents.\n\nQuestion: What is the purpose of CoVoST?\n\nAnswer: The purpose of CoVoST is to provide a multilingual speech-to-text translation corpus for 11 languages into English, diversified with over 11,000 speakers and over 60 accents.\n\nQuestion: What is the difference between corpus-level BLEU and sentence-level BLEU?\n\nAnswer: Corpus-level BLEU provides a normalized quality score as oppose to", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": " They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\nAnswer: They used the Twitter dataset.\n\n", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": " They use large BERT.\n\nQuestion: What is the main reason for the great improvements of their experimental results?\n\nAnswer: The main reason for the great improvements of their experimental results is that they construct context-gloss pairs and convert WSD problem to a sentence-pair classification task which is similar to NLI tasks and train only one classifier, which is equivalent to expanding the corpus.\n\nQuestion: What is the advantage of BERT model in dealing with sentence-pair classification tasks?\n\nAnswer: The advantage of BERT model in dealing with sentence-pair classification tasks is that it shows its advantage in", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": " Yes, the automatically constructed datasets are subject to quality control.\n\nQuestion: What are the potential biases in the expert knowledge used for probing?\n\nAnswer: The potential biases in the expert knowledge used for probing are fallible and error-prone.\n\nQuestion: Can the positive results be taken with a grain of salt?\n\nAnswer: Yes, the positive results can be taken with a grain of salt.\n\nQuestion: What are the trade-offs of using synthetic versus naturalistic QA data?\n\nAnswer: The trade-offs of using synthetic versus naturalistic QA data are that synthetic data allows for", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": " Yes, the images are from the ShapeWorld domain.\n\nQuestion: What is the purpose of the GTD evaluation framework?\n\nAnswer: The purpose of the GTD evaluation framework is to evaluate image captioning models for grammaticality, truthfulness, and diversity.\n\nQuestion: How does the GTD framework evaluate image captioning models?\n\nAnswer: The GTD framework evaluates image captioning models by comparing the candidate captions to a set of reference captions and assessing their grammaticality, truthfulness, and diversity.\n\nQuestion: What are the three criteria for evaluating image captioning models in the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": " Their performance on emotion detection was competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets.\n\nAnswer: Their performance on emotion detection was competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets.\n\nAnswer: Their performance on emotion detection was competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets.\n\nAnswer: Their performance on emotion detection was competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": " No, Arabic is not one of the 11 languages in CoVost.\n", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": " The tagging scheme employed is a simple tagging scheme consisting of two tags: INLINEFORM0 and INLINEFORM1.\n\nQuestion: What does the INLINEFORM0 tag mean?\n\nAnswer: The INLINEFORM0 tag means that the current word is not a pun.\n\nQuestion: What does the INLINEFORM0 tag mean?\n\nAnswer: The INLINEFORM0 tag means that the current word is a pun.\n\nQuestion: What does the INLINEFORM0 tag indicate?\n\nAnswer: The INLINEFORM0 tag indicates that the current word appears before the pun in the given context", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": " Average GloVe embeddings, InferSent BIBREF4, and Universal Sentence Encoder BIBREF5.\n\nQuestion: How does SBERT compare to InferSent in terms of computational efficiency?\n\nAnswer: SBERT is about 65% faster than InferSent.\n\nQuestion: What is the purpose of the siamese network architecture in SBERT?\n\nAnswer: The siamese network architecture enables the derivation of fixed-size vectors for input sentences, allowing for the finding of semantically similar sentences.\n\nQuestion: What is the computational efficiency of S", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " Robustness of a model is defined as the ability of the model to handle unbalanced labeled features and still perform well.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is the introduction of three regularization terms to address the robustness problem in learning models.\n\nQuestion: What is the purpose of the maximum entropy regularization term?\n\nAnswer: The purpose of the maximum entropy regularization term is to control unbalance in the model's predicted class distribution.\n\nQuestion: How does the KL divergence regularization term work?\n\nAnswer: The KL", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": " They test their conflict method on tasks like natural language inference and question answering.\n\nQuestion: What is the objective of attention in their models?\n\nAnswer: The objective of attention in their models is to match one sequence with another.\n\nQuestion: What is the difference between attention and conflict?\n\nAnswer: Attention looks for matching word representations between two sequences, while conflict looks for contrasting relationship between words in two sequences.\n\nQuestion: What is the purpose of the conflict mechanism?\n\nAnswer: The purpose of the conflict mechanism is to capture how two sequences repel each other.\n\nQuestion: How do they", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": " The method's improvements of F1 for NER task for English and Chinese datasets are 84.67 and 84.67, respectively.\n\nAnswer: The method's improvements of F1 for NER task for English and Chinese datasets are 84.67 and 84.67, respectively.\n\nAnswer: The method's improvements of F1 for NER task for English and Chinese datasets are 84.67 and 84.67, respectively.\n\nAnswer: The method's improvements of F1 for NER task for English and Chinese datasets are 84.67 and 84.67, respectively.\n\nAnswer: The method's improvements", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": " The core component for KBQA is the relation detection model.\n\nQuestion: How does the proposed relation detection model improve the KBQA end task?\n\nAnswer: The proposed relation detection model improves the KBQA end task by 2-3% compared to the baseline relation detector.\n\nQuestion: What is the purpose of the entity re-ranking step in the KBQA system?\n\nAnswer: The purpose of the entity re-ranking step is to improve the performance of the KBQA system by re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " They compared against the baseline models BIBREF12, BIBREF5, and BIBREF6.\n\nQuestion: What is the main contribution of their model?\n\nAnswer: The main contribution of their model is that it fully utilizes linguistic priors.\n\nQuestion: What is the purpose of the tag-level tree-LSTM?\n\nAnswer: The purpose of the tag-level tree-LSTM is to control the composition function of the existing word-level tree-LSTM.\n\nQuestion: What is the significance of the clustered tag set?\n\nAnswer: The significance of", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": " The methods considered to find examples of biases and unwarranted inferences are: (1) manually categorizing each of the baby images, (2) tagging all descriptions with part-of-speech information, (3) creating a coreference graph by linking all phrases that refer to the same entity, (4) applying Louvain clustering to the coreference graph, and (5) looking at the clusters of expressions that refer to similar entities.\n\nAnswer: The methods considered to find examples of biases and unwarranted inferences are: (1) manually categorizing each of the baby images, (2) tagging all descriptions", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": " The baseline models are the encoder-decoder model and the hierarchical model.\n\nQuestion: What is the purpose of the personalized models?\n\nAnswer: The purpose of the personalized models is to generate personalized recipes based on user preferences.\n\nQuestion: How do the personalized models generate recipes?\n\nAnswer: The personalized models generate recipes by attending over user preferences extracted from previously consumed recipes.\n\nQuestion: How do the personalized models improve the semantic plausibility of generated recipes?\n\nAnswer: The personalized models improve the semantic plausibility of generated recipes by learning the overall recipe step ordering structure and predicting the entailment score", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": " They experimented with plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections.\n\nAnswer: They experimented with plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections.\n\nAnswer: They experimented with plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections.\n\nAnswer: They experimented with plain stacked LSTMs", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": " They explore the use of Winograd schemas in machine translation programs.\n\nQuestion: What is the purpose of the Winograd Schema Challenge?\n\nAnswer: The purpose of the Winograd Schema Challenge is to test the depth of understanding achieved by machine translation programs.\n\nQuestion: What is the difference between the masculine and feminine third-person plural pronouns in Romance languages?\n\nAnswer: In Romance languages, the masculine pronoun is standardly used for groups of mixed or unknown gender, while the feminine pronoun is used for groups of females.\n\nQuestion: How can Winograd schemas be used", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": " The authors experimented with various summarization algorithms provided by the Sumy package.\n\nQuestion: What is the purpose of the ILP-based summarization technique?\n\nAnswer: The purpose of the ILP-based summarization technique is to produce a summary of peer feedback comments for a given employee.\n\nQuestion: What is the main objective of the sentence classification algorithm?\n\nAnswer: The main objective of the sentence classification algorithm is to identify strengths, weaknesses and suggestions for improvements found in the supervisor assessments.\n\nQuestion: What is the purpose of the multi-class multi-label classification technique?\n\nAnswer:", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": " No, they report results on English data, but the methodology can be applied to other languages as well.\n\nAnswer: Yes, they report results only on English data.\n\nAnswer: Unanswerable.\n\nAnswer: No, they report results on English data, but the methodology can be applied to other languages as well.\n\nAnswer: Yes, they report results only on English data.\n\nAnswer: Unanswerable.\n\nAnswer: No, they report results on English data, but the methodology can be applied to other languages as well.\n\nAnswer: Yes, they report results only on English data", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": " The least impactful component is the one that is not mentioned in the article.\n\nQuestion: What is the main focus of the article?\n\nAnswer: The main focus of the article is the application of message passing framework to natural language processing.\n\nQuestion: What is the purpose of the Message Passing Attention network for Document understanding (MPAD)?\n\nAnswer: The purpose of the Message Passing Attention network for Document understanding (MPAD) is to represent documents as word co-occurrence networks and use a message passing framework to learn representations that are competitive with the state-of-the-art.\n\nQuestion:", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": " The previous state of the art for this task was to aid instructors to selectively intervene on student discussions where they are needed the most.\n\nQuestion: What is the problem statement for predicting instructor intervention in MOOC forums?\n\nAnswer: The problem statement for predicting instructor intervention in MOOC forums is to predict if an instructor has intervened in a thread given the posts in the thread.\n\nQuestion: What is the primary problem that leads to a secondary problem of inferring the appropriate amount of context to intervene?\n\nAnswer: The primary problem is to predict instructor intervention in a thread given the posts in the thread. The secondary problem", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": " The 7 Indian languages they experiment with are Hindi, Bengali, Marathi, Tamil, Telugu, Kannada, and Malayalam.\n\nQuestion: What is the name of the pooling strategy used in this paper?\n\nAnswer: The pooling strategy used in this paper is called Ghost-VLAD.\n\nQuestion: What is the name of the base feature extraction network used in this paper?\n\nAnswer: The base feature extraction network used in this paper is called ResNet.\n\nQuestion: What is the name of the toolkit used to train the i-vector+svm system?", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": " The task uses the DTA corpus pair from BIBREF0: DTA18 and DTA19.\n\nQuestion: What is the goal of the shared task?\n\nAnswer: The goal of the shared task is to create an architecture to detect semantic change and to rank words according to their degree of change between two different time periods.\n\nQuestion: What is the metric used to assess the performance of the models?\n\nAnswer: The metric used to assess the performance of the models is Spearman's $\\rho $.\n\nQuestion: What are the two baselines used to compare the models' performances?", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " The proposed model, ALOHA, achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models.\n\nQuestion: What is the main contribution of the proposed model?\n\nAnswer: The main contribution of the proposed model is the use of Human Level Attributes (HLAs) to model human-like attributes of characters, which is a novel approach.\n\nQuestion: What is the main limitation of the proposed model?\n\nAnswer: The main limitation of the proposed model is that it is limited by the knowledge of HLAs, as it does not perform well when the model", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " The model performance on target language reading comprehension is reasonable.\n\nQuestion: What is the effect of typology variation and code-switching on multi-BERT performance?\n\nAnswer: Typology variation and code-switching only caused minor effects on testing performance.\n\nQuestion: What is the method used to generate datasets?\n\nAnswer: The method used to generate datasets is the same as BIBREF21.\n\nQuestion: What is the effect of changing the English typology order to SOV or OSV order?\n\nAnswer: The effect of changing the English typology order to SOV or OS", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": " The authors present evidence that the model can capture biases in data annotation and collection by examining a subset of the data and recording some of the mislabeled items. They also mention that even for a human, it is difficult to discriminate against implicit abuses. Additionally, they discuss how biases in data collection and annotation rules can lead to misclassifications.\n", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " The experimental results show that ARAML performs significantly better than other baselines in all the cases. This result indicates that the samples surrounding true responses provide stable rewards for the generator, and stable RAML training paradigm significantly enhances the performance in both metrics.\n\nAnswer: The experimental results show that as the temperature becomes larger, forward perplexity increases gradually while Self-BLEU decreases. As mentioned in Section UID13, large temperatures encourage our generator to explore the samples that are distant from real data distribution, thus the diversity of generated results will be improved. However, these samples distant from the data distribution are more likely to be poor in flu", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": " The dataset is almost ten times bigger compared to ILPRL dataset in terms of entities.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the proposed neural based Nepali NER using latest state-of-the-art architecture based on grapheme-level which doesn't require any hand-crafted features and no data pre-processing.\n\nQuestion: What is the effect of stemming post-positions on the model performance?\n\nAnswer: Stemming post-positions can help a lot in improving model performance because of inflectional characteristics of", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": " Yes, we tested several other baselines including a baseline that uses a simple rule-based approach, a baseline that uses a simple statistical approach, and a baseline that uses a more complex statistical approach.\n\nQuestion: What is the performance of the neural baseline on the PrivacyQA corpus?\n\nAnswer: The neural baseline achieves a performance of only 39.8 F1 on the PrivacyQA corpus.\n\nQuestion: What are some of the challenges to answering questions about privacy policies?\n\nAnswer: Some of the challenges include the presence of unanswerable questions, the need for users to read and understand privacy policies,", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " The datasets used in this work are the BIBREF0 dataset, which contains EEG data from a single participant, and the BIBREF1 dataset, which contains EEG data from multiple participants.\n\nQuestion: What is the purpose of the neural network?\n\nAnswer: The purpose of the neural network is to predict the ERP components from the EEG data.\n\nQuestion: What is the relationship between the ERP components and the behavioral data?\n\nAnswer: The ERP components can be predicted from the neural network, and the neural network can predict the ERP components. Additionally, the neural network can predict the behavioral", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " The proposed method improves the F1 score for paraphrase identification.\n\nAnswer: The proposed method does not improve the F1 score for paraphrase identification.\n\nAnswer: The proposed method improves the F1 score for paraphrase identification.\n\nAnswer: The proposed method does not improve the F1 score for paraphrase identification.\n\nAnswer: The proposed method improves the F1 score for paraphrase identification.\n\nAnswer: The proposed method does not improve the F1 score for paraphrase identification.\n\nAnswer: The proposed method improves the F1 score for paraphrase identification.\n\nAnswer: The proposed method", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " The baselines used for evaluation are the summarization style headline “Measuring Trump’s Media Dominance” and the sensational headline “$2 Billion Worth of Free Media for Trump”.\n\nAnswer: The baselines used for evaluation are the summarization style headline “Measuring Trump’s Media Dominance” and the sensational headline “$2 Billion Worth of Free Media for Trump”.\n\nAnswer: The baselines used for evaluation are the summarization style headline “Measuring Trump’s Media Dominance” and the sensational headline “$2 Billion", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": " The subjects were presented with a series of visual stimuli, such as words, pictures, and geometric shapes, to elicit event-related responses.\n\nQuestion: What is the purpose of the mixed neural network architecture described in the article?\n\nAnswer: The mixed neural network architecture is designed to improve the classification accuracy of EEG data based on speech imagery by combining the feature extraction capabilities of CNN, LSTM, and deep autoencoders.\n\nQuestion: How does the proposed method compare to previous approaches in terms of accuracy and reliability?\n\nAnswer: The proposed method outperforms previous approaches in terms of accuracy and reliability,", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": " The article discusses the use of pre-trained language models in sequence to sequence models for tasks such as neural machine translation and abstractive document summarization. The article specifically mentions the use of strong transformer-based language models trained on up to six billion tokens.\n\nQuestion: How do pre-trained language models improve the performance of sequence to sequence models?\n\nAnswer: Pre-trained language models provide large improvements for a range of language understanding tasks. They are applied to sequence to sequence models by either inputting ELMo-style representations to the encoder or by fine-tuning the language model. These strategies help the models to", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": " The learning models used on the dataset are traditional machine learning classifiers and neural network models.\n\nQuestion: What is the most accurate learning model in classifying abusive language?\n\nAnswer: The most accurate learning model in classifying abusive language is the Bidirectional GRU network with LTC.\n\nQuestion: How do character-level features affect the accuracy of neural network models?\n\nAnswer: Character-level features reduce the accuracy of neural network models.\n\nQuestion: What is the purpose of using context tweets in neural network models?\n\nAnswer: The purpose of using context tweets in neural network models is to", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": " The results from these proposed strategies show that the knowledge graph based state-representation is critical in aiding exploration and surpassing the bottleneck in the game. The Go-Explore based exploration algorithm sees less of a difference between agents, with KG-A2C-chained being significantly more sample efficient and converging faster.\n", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": " We associate each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds. This strategy helps to deemphasize confident examples during training as their $p$ approaches the value of 1, makes the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples.\n\nQuestion: What is the highest F1 for Chinese OntoNotes4.0?\n\nAnswer: The highest F1 for Chinese OntoNotes4.0 is 84.67 when $\\alpha $ is set to 0.6.\n\nQuestion: What is the", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": " Non-standard pronunciation is identified by the system.\n\nQuestion: What is the purpose of the resource?\n\nAnswer: The purpose of the resource is to provide a larger data set and baselines for NLP tasks in Mapudungun.\n\nQuestion: What is the significance of the resource for the Mapuche community?\n\nAnswer: The resource has the potential to alleviate many of the issues faced when building language technologies for Mapudungun, in contrast to other indigenous languages of the Americas that remain low-resource.\n\nQuestion: What is the difference between Mapudungun and other polysynt", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": " An individual model consists of a Bayesian model for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents.\n\nQuestion: What is the purpose of using crosslingual latent variables in the model?\n\nAnswer: The crosslingual latent variables capture role alignments in parallel corpora, which helps improve the performance of the model.\n\nQuestion: How does the model handle the task of unsupervised semantic role induction?\n\nAnswer: The model consists of a standard unsupervised SRL setup, where the task of unsupervised semantic role induction is the fourth step.\n\nQuestion", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": " The article explores 16 different languages.\n\nQuestion: what is the main goal of the article?\n\nAnswer: The main goal of the article is to compare the impact of external lexicons and word vector representations on the accuracy of PoS models.\n\nQuestion: what is the difference between word vectors and lexical resources?\n\nAnswer: Word vectors are built in an unsupervised way, only requiring large amounts of raw textual data. They also encode finer-grained information than usual morphosyntactic lexicons, most of which do not include any quantitative data, not even simple frequency information. Conversely, lexical", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": " A semicharacter architecture is a type of architecture used in deep learning techniques that combines both character-level and word-level information. It is used to improve the performance of models on diverse supervised learning tasks.\n\nQuestion: What is the purpose of adversarial attacks?\n\nAnswer: The purpose of adversarial attacks is to manipulate the input data in a way that causes a model to make incorrect predictions. This is done to test the robustness of the model and identify vulnerabilities that can be exploited.\n\nQuestion: How do adversarial misspellings affect text classification models?\n\nAnswer: Adversarial misspell", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": " Yes, the data is de-identified.\n\nAnswer: No, the data is not de-identified.\n\nAnswer: Unanswerable.\n\nAnswer: Yes.\n\nAnswer: No.\n\nAnswer: Unanswerable.\n\nAnswer: Yes.\n\nAnswer: No.\n\nAnswer: Unanswerable.\n\nAnswer: Yes.\n\nAnswer: No.\n\nAnswer: Unanswerable.\n\nAnswer: Yes.\n\nAnswer: No.\n\nAnswer: Unanswerable.\n\nAnswer: Yes.\n\nAnswer: No.\n\nAnswer: Un", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " Their NCEL approach is effective overall.\n\nEntity linking (EL), mapping entity mentions in texts to a given knowledge base (KB), serves as a fundamental role in many fields, such as question answering, semantic search, and information extraction. However, this task is non-trivial because entity mentions are usually ambiguous. As shown in Figure FIGREF1, the mention England refers to three entities in KB, and an entity linking system should be capable of identifying the correct entity as England cricket team rather than England and England national football team.\n\nEntity linking is typically broken down into two main phases: (i) candidate generation", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": " The authors obtained the annotated clinical notes from visualDx, a healthcare informatics company that provides web-based clinical decision support system.\n\nAnswer: The authors obtained the annotated clinical notes from visualDx, a healthcare informatics company that provides web-based clinical decision support system.\n\nAnswer: The authors obtained the annotated clinical notes from visualDx, a healthcare informatics company that provides web-based clinical decision support system.\n\nAnswer: The authors obtained the annotated clinical notes from visualDx, a healthcare informatics company that provides web-based clinical decision support system.\n\nAnswer:", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": " The baseline used was the system by Felice2014a.\n\nQuestion: What was the improvement achieved by using artificial data?\n\nAnswer: The addition of artificial data provided significant improvements over using only the available training set.\n\nQuestion: What were the two alternative methods for AEG?\n\nAnswer: The two alternative methods for AEG were a phrase-based SMT error generation system and a method for learning error patterns from an annotated corpus.\n\nQuestion: What was the evaluation measure used?\n\nAnswer: The evaluation measure used was INLINEFORM0, which was established as the preferred measure for error", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": " They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\nAnswer: They use the Twitter dataset.\n\n", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": " Masking words in the decoder is helpful because it allows the decoder to learn summary representations, context interactions and language modeling together.\n\nQuestion: What is the main contribution of this work?\n\nAnswer: The main contribution of this work is to propose a natural language generation model based on BERT, making good use of the pre-trained language model in the encoder and decoder process, and the model can be trained end-to-end without handcrafted features.\n\nQuestion: What is the difference between extractive and abstractive summarization?\n\nAnswer: Extractive summarization generates summary by selecting salient", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": " The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\"), disturbed sleep (e.g., “another restless night\"), or fatigue or loss of energy (e.g., “the fatigue is unbearable", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": " TF-IDF features are used.\n\nQuestion: What is the performance of the XGBoost classifier?\n\nAnswer: The XGBoost classifier outperformed all other models for both the micro F-score metric, with a score of 0.92, and the macro F-score metric, with a score of 0.31.\n\nQuestion: What is the performance of SVM with linear kernel?\n\nAnswer: SVM with linear kernel performs much better than SVM with RBF kernel, scoring 9% on the macro F-score and 12% more on the micro F-score", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": " The training data was translated using the machine translation platform Apertium.\n\nQuestion: What is the purpose of creating word embeddings?\n\nAnswer: The purpose of creating word embeddings is to train word embeddings for word embeddings.\n\nQuestion: What is the difference between the regular and translated columns in Tables TABREF17 and TABREF18?\n\nAnswer: The difference between the regular and translated columns in Tables TABREF17 and TABREF18 is that the regular columns contain the English versions of the lexicons, while the translated columns contain the Spanish versions of the lex", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": " They evaluated on eight out of eight biomedical NER tasks.\n\nQuestion: What is the name of the model they used for domain adaptation?\n\nAnswer: They used BioBERT.\n\nQuestion: What is the name of the dataset they used for Covid-19 QA?\n\nAnswer: They used PubMed+PMC and/or CORD-19.\n\nQuestion: What is the name of the model they used for Covid-19 QA?\n\nAnswer: They used BERT.\n\nQuestion: What is the name of the model they used for domain adaptation?\n\nAnswer", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": " The baseline for this task was a system that achieved an F$_1$ score of 0.5 on the development set.\n\nAnswer: The baseline for this task was a system that achieved an F$_1$ score of 0.5 on the development set.\n\nAnswer: The baseline for this task was a system that achieved an F$_1$ score of 0.5 on the development set.\n\nAnswer: The baseline for this task was a system that achieved an F$_1$ score of 0.5 on the development set.\n\nAnswer: The baseline for this task was a system", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": " They used a content-based classifier.\n\nQuestion: What was the overall accuracy of their system?\n\nAnswer: The overall accuracy was up to 0.534.\n\nQuestion: Did they find any correlation between the usage of positive and negative emotional words and the gender dominance in an industry?\n\nAnswer: No, they did not find any statistically significant correlation.\n\nQuestion: What did they find when they compared the industry rankings of the relative frequencies of emotionally charged words for the two genders?\n\nAnswer: They found that the industry rankings were statistically significantly correlated.\n\nQuestion: What did they find", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " The political bias of different sources is not explicitly included in the model. The authors focus on the typology of content shared rather than the political bias of news sources.\n\nAnswer: The political bias of different sources is not explicitly included in the model. The authors focus on the typology of content shared rather than the political bias of news sources.\n\nAnswer: The political bias of different sources is not explicitly included in the model. The authors focus on the typology of content shared rather than the political bias of news sources.\n\nAnswer: The political bias of different sources is not explicitly included in the model. The authors focus", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": " They compare with the state-of-the-art systems for homographic puns.\n\nQuestion: What is the purpose of the pun detection and location tasks?\n\nAnswer: The purpose is to detect whether a sentence contains a pun and to find the exact word in the sentence that triggers more than one semantic interpretations.\n\nQuestion: What is the difference between heterographic puns and homographic puns?\n\nAnswer: Heterographic puns exploit the sound similarity between the pun and its latent target, while homographic puns reflect the two distinct senses of the pun.\n\nQuestion: What is the main", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": " The tweets are in English.\n\nQuestion: What is the purpose of the dataset?\n\nAnswer: The purpose of the dataset is to train systems capable of recognizing offensive content and to provide a new dataset for research in the field of offensive language detection.\n\nQuestion: What are the three levels of annotation in OLID?\n\nAnswer: The three levels of annotation in OLID are type, target, and offensive language.\n\nQuestion: What is the performance of the CNN system in the offensive language target identification experiment?\n\nAnswer: The performance of the CNN system in the offensive language target identification experiment is 0.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": " The ancient Chinese dataset comes from the proposed ancient-modern Chinese text alignment method at the level of clause based on the characteristics of these two languages.\n\nQuestion: What is the purpose of the ancient-modern Chinese translation dataset?\n\nAnswer: The purpose of the ancient-modern Chinese translation dataset is to provide a large high-quality ancient-modern Chinese dataset for testing the performance of SMT and various NMT models.\n\nQuestion: What are the two types of methods used to acquire translation examples?\n\nAnswer: The two types of methods used to acquire translation examples are lexical-based and statistical-based.\n", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " The UTCNN model has three layers.\n\nQuestion: What is the purpose of the user embeddings in the UTCNN model?\n\nAnswer: The user embeddings in the UTCNN model capture the user's interactions and stances.\n\nQuestion: What is the advantage of using the PSL model for stance classification?\n\nAnswer: The PSL model can jointly label the stances of authors and posts.\n\nQuestion: What is the main finding of the study on the CreateDebate dataset?\n\nAnswer: The main finding of the study on the CreateDebate dataset is that the UTCNN model achieves", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": " The article uses the PTB dataset for training and evaluation.\n\nQuestion: what is the difference between the neural PCFG and the compound PCFG?\n\nAnswer: The neural PCFG uses a neural network to parameterize the rule probabilities, while the compound PCFG uses a sentence-level continuous latent vector.\n\nQuestion: what is the purpose of the discriminative parser?\n\nAnswer: The discriminative parser is used to optimize the evidence lower bound for the RNNG.\n\nQuestion: what is the difference between the RNNG and the PRPN/ON baselines?\n\nAnswer:", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": " The paper uses two datasets: the MEDDOCAN: Medical Document Anonymization shared task dataset and NUBes BIBREF4, a corpus of real medical reports in Spanish.\n\nQuestion: What is the purpose of the experiments conducted in the paper?\n\nAnswer: The experiments aim to evaluate the performance of a BERT-based sequence labelling approach for Spanish clinical text anonymisation.\n\nQuestion: What is the main finding of the experiments?\n\nAnswer: The experiments show that the BERT-based model outperforms other systems without requiring any adaptation or domain-specific feature engineering, just by being trained on", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": " The dataset used in this paper is Flickr.\n\nQuestion: what is the main hypothesis of this paper?\n\nAnswer: The main hypothesis of this paper is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main motivation for using vector space embeddings in this paper?\n\nAnswer: The main motivation for using vector space embeddings in this paper is that they allow us to integrate the textual information we get from Flickr with available structured information in a very natural", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": " The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the average F1 score and the user response rate.\n\nQuestion: What is the main limitation of existing chat systems in terms of knowledge learning?\n\nAnswer: The main limitation of existing chat systems in terms of knowledge learning is that they do not explicitly or implicitly learn new knowledge in the conversation process.\n\nQuestion: What is the difference between KBC and OKBC?\n\nAnswer: KBC assumes that all INLINEFORM0, INLINEFORM1, and INLINEFORM2 are known to exist in the KB,", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " They used Unigrams and Pragmatic features BIBREF0, BIBREF1, BIBREF2, BIBREF3, Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5.\n\nAnswer: They used Unigrams and Pragmatic features BIBREF0, BIBREF1, BIBREF2, BIBREF3, Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5.\n\nAnswer: They used Unigrams and Pragmatic features BIBREF0, BIBREF1, BIB", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The targets of the annotated stances in the tweet data set are two popular sports clubs in Turkey.\n\nQuestion: What is the purpose of the stance detection system?\n\nAnswer: The purpose of the stance detection system is to detect stance within text towards a target, which may be explicitly specified in the text or not.\n\nQuestion: What are the two stance classes in the tweet data set?\n\nAnswer: The two stance classes in the tweet data set are Favor and Against.\n\nQuestion: What are the features used in the SVM classifiers?\n\nAnswer: The features used in the SVM class", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": " Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset.\n\nQuestion: What is the purpose of the corpus created by the authors?\n\nAnswer: The purpose of the corpus created by the authors is to evaluate the robustness of statistical question answering learning.\n\nQuestion: What is the name of the dataset that consists of 277 questions?\n\nAnswer: The name of the dataset that consists of 277 questions is QASent.\n\nQuestion: What is the name of the dataset that consists of 15K+ questions on insurance contexts?\n\nAnswer: The name of the", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": " Gaussian-masked directional multi-head attention is a variant of self-attention that allows the self-attention layer to generate the representation of sentences at once and contain the information of the sentence which is different from RNN that process characters of sentences one by one. Standard self-attention is similar as Gaussian-masked direction attention while it does not have directional mask and gaussian mask. BIBREF24 also propose multi-head attention which is better to generate representation of sentence by dividing queries, keys and values to different heads and get information from different subspaces.\n\nQuestion: What is the difference", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences. Sometimes ironies are hard to understand and may cause misunderstanding, for which our task also explores the transformation from ironic sentences to non-ironic sentences.\n\nAnswer: As mentioned above, many style transfer models, such as DualRL, tend to make few changes to the input sentence and output the same sentence. Actually, this is a common issue for unsupervised style transfer systems and we also meet it during our experiments. The main reason for the issue is that rewards for content preservation are too prominent and rewards for style accuracy cannot work well. In", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": " The network's baseline features are the word2vec embeddings.\n\nQuestion: What is the purpose of using pre-trained features in sarcasm detection?\n\nAnswer: The purpose of using pre-trained features in sarcasm detection is to extract contextual information and improve the generalizability of the model.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is the development of pre-trained sentiment, emotion and personality models for sarcasm detection using CNN.\n\nQuestion: What is the main limitation of the state-of-the-art methods in sarc", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": " They considered social media text.\n\nQuestion: What were the two subtasks of causal explanation analysis?\n\nAnswer: The two subtasks were causality prediction and causal explanation identification.\n\nQuestion: What was the best performing model for causality prediction?\n\nAnswer: The best performing model for causality prediction was an SVM.\n\nQuestion: What was the best performing model for causal explanation identification?\n\nAnswer: The best performing model for causal explanation identification was a hierarchy of BiLSTMs.\n\nQuestion: What was the overall accuracy of the pipeline for causal explanation analysis?\n\nAnswer: The", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": " Their system ranked second, second, fourth, and fifth place.\n\nQuestion: What were the scores of their system?\n\nAnswer: Their system ranked second, second, fourth, and fifth place.\n\nQuestion: What were the scores of their system?\n\nAnswer: Their system ranked second, second, fourth, and fifth place.\n\nQuestion: What were the scores of their system?\n\nAnswer: Their system ranked second, second, fourth, and fifth place.\n\nQuestion: What were the scores of their system?\n\nAnswer: Their system ranked second, second, fourth, and fifth place", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": " Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": " The hyperparameters that were varied in the experiments on the four tasks were the number of clusters (INLINEFORM1) and the dimensionality of the word embeddings (INLINEFORM0).\n\nAnswer: The hyperparameters that were varied in the experiments on the four tasks were the number of clusters (INLINEFORM1) and the dimensionality of the word embeddings (INLINEFORM0).\n\nAnswer: The hyperparameters that were varied in the experiments on the four tasks were the number of clusters (INLINEFORM1) and the dimensionality of the word embeddings (INLINE", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": " Their model is compared to previous methods such as rule-based pattern matching, machine learning approaches, and learned methods.\n\nQuestion: What is the highest performing question classification system?\n\nAnswer: The highest performing question classification system is a single learned model.\n\nQuestion: What is the challenge of hypernym expansion?\n\nAnswer: The challenge of hypernym expansion is determining a heuristic for the termination depth of hypernym expansion.\n\nQuestion: What is the benefit of using Probase hypernyms?\n\nAnswer: The benefit of using Probase hypernyms is that they come from use in", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": " The corpus contains over 10,000 case reports.\n\nQuestion: What are the five distinct entity types annotated in the corpus?\n\nAnswer: The five distinct entity types annotated in the corpus are cases, conditions, factors, findings, and modifiers.\n\nQuestion: What is the purpose of the annotation guidelines?\n\nAnswer: The annotation guidelines provide instructions on how to annotate the entities in the corpus.\n\nQuestion: What is the purpose of the evaluation?\n\nAnswer: The purpose of the evaluation is to compare the performance of different NER systems on the corpus.\n\nQuestion: What is the", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": " The dataset contains a total of 5,000 sentences.\n\nQuestion: What is the purpose of the dataset?\n\nAnswer: The purpose of the dataset is to support future research in the field of named entity recognition for the Nepali language.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the proposal of a neural based named entity recognition model for the Nepali language.\n\nQuestion: What are some of the factors that affect the performance of the NER model?\n\nAnswer: Some of the factors that affect the performance of the NER model are", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": " They consider text categorization, sentiment classification, and topic modeling.\n\nQuestion: What is the main problem they address in this paper?\n\nAnswer: The main problem they address is how to leverage prior knowledge robustly in learning models.\n\nQuestion: What are the three regularization terms they propose?\n\nAnswer: The three regularization terms they propose are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.\n\nQuestion: How do they incorporate neutral features into their regular", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": " Yes, their NER model learns NER from both text and images.\n\nQuestion: Does their NER model outperform traditional text-based NER baselines?\n\nAnswer: Yes, their NER model outperforms traditional text-based NER baselines.\n\nQuestion: Does their NER model use a general modality attention module?\n\nAnswer: Yes, their NER model uses a general modality attention module.\n\nQuestion: Does their NER model improve the model's robustness to missing tokens?\n\nAnswer: Yes, their NER model improves the model's robustness to missing", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": " The training sets of these versions of ELMo are larger than the previous ones.\n\nQuestion: What is the purpose of the ELMo model?\n\nAnswer: The purpose of the ELMo model is to capture the context of words and provide better semantic representations.\n\nQuestion: What is the difference between word2vec and ELMo embeddings?\n\nAnswer: Word2vec embeddings do not capture the context of words, while ELMo embeddings do.\n\nQuestion: What is the advantage of using contextual embeddings?\n\nAnswer: Contextual embeddings provide better semantic representations and", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": " Their highest MRR score was 0.4325.\n\nQuestion: What is the name of the system that achieved the highest recall score in the fourth test batch set?\n\nAnswer: The name of the system that achieved the highest recall score in the fourth test batch set is BioBERT BIBREF1.\n\nQuestion: What is the name of the competition that they participated in?\n\nAnswer: The name of the competition that they participated in is BioASQ.\n\nQuestion: What is the name of the model that they used for their system?\n\nAnswer: The name of the model that they", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": " They compare their approach to the s2sL approach.\n\nQuestion: What is the proposed approach?\n\nAnswer: The proposed approach is called simultaneous two sample learning (s2sL).\n\nQuestion: What is the data representation format used in their approach?\n\nAnswer: The data representation format used in their approach is called simultaneous two sample (s2s) representation.\n\nQuestion: What is the purpose of the s2s representation?\n\nAnswer: The purpose of the s2s representation is to simultaneously consider two samples and help the network learn the characteristics of the two classes separately, as well", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": " The authors evidence this claim by stating that engineers often face the following challenges when applying DNN models to address specific NLP tasks, such as text classification, sequence labeling, question answering, etc. These challenges often hinder the productivity of engineers, and result in less optimal solutions to their given tasks. This motivates them to develop an NLP toolkit for DNN models, which facilitates engineers to develop DNN approaches. Before designing this NLP toolkit, they conducted a survey among engineers and identified a spectrum of three typical personas. To satisfy the requirements of all the above three personas, the NLP toolkit has to be", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": " No, they evaluate on both English and non-English datasets.\n\nQuestion: What is the purpose of using pre-trained word embeddings?\n\nAnswer: The purpose of using pre-trained word embeddings is to inject a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories.\n\nQuestion: How do they learn latent embeddings?\n\nAnswer: They learn latent embeddings by jointly learning a new latent embedding space as a projection of pre-trained embeddings, while also learning latent syntactic structure.\n\nQuestion: What is the advantage of", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": " They evaluate on the Penn Treebank dataset.\n\nQuestion: What is the purpose of using pre-trained word embeddings?\n\nAnswer: The purpose of using pre-trained word embeddings is to inject a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories.\n\nQuestion: What is the difference between the Markov-structured model and the tree-structured model?\n\nAnswer: The Markov-structured model treats a sequence of pre-trained word embeddings as random initialization, while the tree-structured model uses a tree-structured syntax", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": " The system achieves state-of-the-art results on both single-relation and multi-relation KBQA tasks.\n\nQuestion: What is the main focus of this work?\n\nAnswer: The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.\n\nQuestion: What are the three main problems that the proposed improved relation detection model aims to address?\n\nAnswer: The proposed improved relation detection model aims to address the problems of unseen relations, relation names that can help match longer question contexts, and the need to predict a chain of", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
